{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_from_data_frame(data_path):\n",
    "  data_frame = pickle.loads(open(os.path.join(cwd, data_path), 'rb').read())\n",
    "    \n",
    "  tweet_list = data_frame['tweets'].tolist()\n",
    "  aspect_list = data_frame['annotated_aspects'].tolist()\n",
    "  label = data_frame['sentence_label'].tolist()\n",
    "  return tweet_list,aspect_list,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_level_list(path):\n",
    "  '''\n",
    "    Takes the path of annotated json file and \n",
    "    returns the texts as list and the aspects with annotated labels\n",
    "  '''\n",
    "  annotated_file = json.loads(open(os.path.join(cwd, path), 'r', encoding=\"utf-8\").read())\n",
    "  tweet_list = list()\n",
    "  label_list = list()\n",
    "\n",
    "  for i in range(len(annotated_file)):\n",
    "    line = annotated_file[i] #dictionary\n",
    "    tweet = line.get(\"data\") #string\n",
    "    tweet_label = line.get(\"label\") #list\n",
    "    tweet_label = tweet_label[0]\n",
    "    \n",
    "    tweet_list.append(tweet)\n",
    "    label_list.append(tweet_label)\n",
    "  \n",
    "  return tweet_list,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def get_list_aspect(data_path):\n",
    "  '''\n",
    "    Takes the path of aspect level annotated json file and \n",
    "    returns the texts as list and the aspects with annotated labels\n",
    "    \n",
    "    Data Structures\n",
    "    ---------------\n",
    "    Input:\n",
    "      data_path   : STRING\n",
    "    Returns:\n",
    "      tweet_list  : LIST\n",
    "      aspect_list : LIST\n",
    "  '''\n",
    "  annotated_file = json.loads(open(os.path.join(cwd, data_path), 'r', encoding=\"utf-8\").read())\n",
    "  tweet_list = []\n",
    "  aspect_list = []\n",
    "\n",
    "  for i in range(len(annotated_file)):\n",
    "    line = annotated_file[i] #dictionary\n",
    "    tweet = line.get(\"data\") #string\n",
    "    tweet_aspect_label = line.get(\"label\") #list\n",
    "\n",
    "    aspect_label_inner_list = []\n",
    "\n",
    "    for j in range(len(tweet_aspect_label)):\n",
    "      aspect_label = tweet_aspect_label[j] #element\n",
    "      \n",
    "      start = aspect_label[0]\n",
    "      end = aspect_label[1]\n",
    "      temp_tweet = tweet[0:start]\n",
    "      \n",
    "      count,has_emoji = check_emoji(temp_tweet)\n",
    "            \n",
    "      if has_emoji:\n",
    "        start -= count\n",
    "        end -= count\n",
    "      else:\n",
    "        start = aspect_label[0]\n",
    "        end = aspect_label[1]\n",
    "      \n",
    "      label = aspect_label[2]\n",
    "      \n",
    "      if label == \"pos\":\n",
    "        label = \"positive\"\n",
    "      elif label == \"neg\":\n",
    "        label = \"negative\"\n",
    "      elif label == \"neu\":\n",
    "        label = \"neutral\"\n",
    "      \n",
    "      aspect = tweet[start:end].strip(\" \").lower()\n",
    "      if (tweet[start:end+1].strip(\" \").lower()) == aspect + \"s\":\n",
    "        aspect = aspect + \"s\"\n",
    "      if (tweet[start:end+2].strip(\" \").lower()) == aspect + \"es\":\n",
    "        aspect = aspect + \"es\"\n",
    "      if (tweet[start:end+3].strip(\" \").lower()) == aspect + \"ren\":\n",
    "        aspect = aspect + \"ren\"\n",
    "\n",
    "      label_dict = {aspect : label}\n",
    "      aspect_label_inner_list.append(label_dict)\n",
    "      \n",
    "    aspect_list.append(aspect_label_inner_list)\n",
    "    tweet_list.append(tweet)\n",
    "  \n",
    "  return tweet_list,aspect_list\n",
    "\n",
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "\n",
    "def check_emoji(data):\n",
    "  tokens = data.split()\n",
    "  has_emoji = False\n",
    "  count = 0\n",
    "  \n",
    "  for token in tokens:\n",
    "    temp_token = re.sub(emoj, '', token)\n",
    "    \n",
    "    if temp_token == '' or len(temp_token) != len(token):\n",
    "      count += 1\n",
    "      has_emoji = True\n",
    "    \n",
    "  return count,has_emoji\n",
    "\n",
    "def remove_excess_space(data):\n",
    "  text = str(data)\n",
    "  text = re.sub(r\"//t\",r\"\\t\", text)\n",
    "  text = re.sub(r\"( )\\1+\",r\"\\1\", text)\n",
    "  text = re.sub(r\"(\\n)\\1+\",r\"\\1\", text)\n",
    "  text = re.sub(r\"(\\r)\\1+\",r\"\\1\", text)\n",
    "  text = re.sub(r\"(\\t)\\1+\",r\"\\1\", text)\n",
    "  return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_al_annotation(al_file_path):\n",
    "  text_list, aspect_dict_list = get_list_aspect(al_file_path)\n",
    "  return text_list,aspect_dict_list\n",
    "\n",
    "def get_sl_annotation(sl_file_path):\n",
    "  text_list, aspect_dict_list = get_sentence_level_list(sl_file_path)\n",
    "  return text_list,aspect_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_text_list_1,sl_label_1 = get_sl_annotation(\"../../data/annotated/7/a1/sl_annotator_1.json\")\n",
    "sl_text_list_2,sl_label_2 = get_sl_annotation(\"../../data/annotated/7/a2/sl_annotator_2.json\")\n",
    "sl_text_list_3,sl_label_3 = get_sl_annotation(\"../../data/annotated/7/a3/sl_annotator_3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "al_text_list_1,al_aspect_dict_list_1 = get_al_annotation(\"../../data/annotated/7/a1/al_annotator_1.json\")\n",
    "al_text_list_2,al_aspect_dict_list_2 = get_al_annotation(\"../../data/annotated/7/a2/al_annotator_2.json\")\n",
    "al_text_list_3,al_aspect_dict_list_3 = get_al_annotation(\"../../data/annotated/7/al.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['tweets'] = al_text_list_3\n",
    "data_frame['annotated_aspects_1'] = al_aspect_dict_list_1\n",
    "data_frame['annotated_aspects_2'] = al_aspect_dict_list_2\n",
    "data_frame['annotated_aspects_3'] = al_aspect_dict_list_3\n",
    "data_frame['sentence_label_1'] = sl_label_1\n",
    "data_frame['sentence_label_2'] = sl_label_2\n",
    "data_frame['sentence_label_3'] = sl_label_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.to_pickle('../data/annotated/7/dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test = train_test_split(data_frame, test_size=0.3, train_size=0.7, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train[['tweets','annotated_aspects_1', 'sentence_label_1']]\n",
    "train_2 = train[['tweets','annotated_aspects_2', 'sentence_label_2']]\n",
    "train_3 = train[['tweets','annotated_aspects_3', 'sentence_label_3']]\n",
    "\n",
    "test_1 = test[['tweets','annotated_aspects_1', 'sentence_label_1']]\n",
    "test_2 = test[['tweets','annotated_aspects_2', 'sentence_label_2']]\n",
    "test_3 = test[['tweets','annotated_aspects_3', 'sentence_label_3']]\n",
    "\n",
    "train_1 = train_1.rename(columns={\"annotated_aspects_1\":\"annotated_aspects\",\"sentence_label_1\": \"sentence_label\"})\n",
    "train_2 = train_2.rename(columns={\"annotated_aspects_2\":\"annotated_aspects\",\"sentence_label_2\": \"sentence_label\"})\n",
    "train_3 = train_3.rename(columns={\"annotated_aspects_3\":\"annotated_aspects\",\"sentence_label_3\": \"sentence_label\"})\n",
    "test_1 = test_1.rename(columns={\"annotated_aspects_1\":\"annotated_aspects\",\"sentence_label_1\": \"sentence_label\"})\n",
    "test_2 = test_2.rename(columns={\"annotated_aspects_2\":\"annotated_aspects\",\"sentence_label_2\": \"sentence_label\"})\n",
    "test_3 = test_3.rename(columns={\"annotated_aspects_3\":\"annotated_aspects\",\"sentence_label_3\": \"sentence_label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1.to_pickle('../../data/annotated/7/a1/train.pkl')\n",
    "train_2.to_pickle('../../data/annotated/7/a2/train.pkl')\n",
    "train_3.to_pickle('../../data/annotated/7/a3/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1.to_pickle('../../data/annotated/7/a1/test.pkl')\n",
    "test_2.to_pickle('../../data/annotated/7/a2/test.pkl')\n",
    "test_3.to_pickle('../../data/annotated/7/a3/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "from scripts import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_list, train_aspect_dict_list,train_label = get_list_from_data_frame('../data/annotated/6/train.pkl')\n",
    "train_text_list = preprocessing.preprocess(train_text_list)\n",
    "joblib.dump(train_text_list, 'data/annotated/7/preprocessed_train_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list, test_aspect_dict_list,test_label = get_list_from_data_frame('../data/annotated/6/test.pkl')\n",
    "test_text_list = preprocessing.preprocess(test_text_list)\n",
    "joblib.dump(test_text_list, 'data/annotated/7/preprocessed_test_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "310a519b676ac3702437d902e068b22df7f36fb88bfa9c8455f322a7fb8ed542"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
