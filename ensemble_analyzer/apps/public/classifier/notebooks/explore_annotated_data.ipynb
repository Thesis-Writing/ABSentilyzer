{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Thesis\\classifier\\classifier\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count annotated aspects (aspect level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", re.UNICODE)\n",
    "\n",
    "def get_list_aspect():\n",
    "  data_path = \"../../data/annotated/7/a2/al_annotator_2.json\"\n",
    "  annotated_file = json.loads(open(os.path.join(cwd, data_path), 'r', encoding=\"utf-8\").read())\n",
    "\n",
    "  aspects = []\n",
    "  tweet_count = 0\n",
    "  count_unique = 0\n",
    "  count_aspects = 0\n",
    "  pos_count = 0\n",
    "  neg_count = 0\n",
    "  neu_count = 0\n",
    "\n",
    "  for i in range(len(annotated_file)):\n",
    "    line = annotated_file[i] #dictionary\n",
    "    tweet = line.get(\"data\") #string\n",
    "    tweet_aspect_label = line.get(\"label\") #list\n",
    "\n",
    "    tweet_count += 1\n",
    "\n",
    "    for j in range(len(tweet_aspect_label)):\n",
    "      aspect_label = tweet_aspect_label[j] #element\n",
    "      \n",
    "      start = aspect_label[0]\n",
    "      end = aspect_label[1]\n",
    "      temp_tweet = tweet[0:start]\n",
    "\n",
    "      count,has_emoji = check_emoji(temp_tweet)\n",
    "\n",
    "      if has_emoji:\n",
    "        start -= count\n",
    "        end -= count\n",
    "      else:\n",
    "        start = aspect_label[0]\n",
    "        end = aspect_label[1]\n",
    "\n",
    "      label = aspect_label[2]\n",
    "\n",
    "      if label == \"pos\":\n",
    "        pos_count += 1\n",
    "        label = \"positive\"\n",
    "      elif label == \"neg\":\n",
    "        neg_count += 1\n",
    "        label = \"negative\"\n",
    "      elif label == \"neu\":\n",
    "        neu_count += 1\n",
    "        label = \"neutral\"\n",
    "\n",
    "      aspect = tweet[start:end].strip(\" \").lower()\n",
    "      if (tweet[start:end+1].strip(\" \").lower()) == aspect + \"s\":\n",
    "        aspect = aspect + \"s\"\n",
    "      if (tweet[start:end+2].strip(\" \").lower()) == aspect + \"es\":\n",
    "        aspect = aspect + \"es\"\n",
    "      if (tweet[start:end+3].strip(\" \").lower()) == aspect + \"ren\":\n",
    "        aspect = aspect + \"ren\"\n",
    "\n",
    "      if aspect not in aspects:\n",
    "        aspects.append(aspect)\n",
    "        count_unique += 1\n",
    "\n",
    "      count_aspects += 1\n",
    "\n",
    "  return tweet_count,pos_count,neg_count,neu_count,count_unique,count_aspects\n",
    "\n",
    "def check_emoji(data):\n",
    "  tokens = data.split()\n",
    "  has_emoji = False\n",
    "  count = 0\n",
    "  \n",
    "  for token in tokens:\n",
    "    temp_token = re.sub(emoj, '', token)\n",
    "    \n",
    "    if temp_token == '' or len(temp_token) != len(token):\n",
    "      count += 1\n",
    "      has_emoji = True\n",
    "    \n",
    "  return count,has_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count,pos_asp_count,neg_asp_count,neu_asp_count,count_unique,count_aspects = get_list_aspect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('translated.txt', 'w', encoding='utf-8') as f:\n",
    "#     for line in text_list:\n",
    "#         f.write(line)\n",
    "#         f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count annotated tweets (sentence level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_level_list():\n",
    "  '''\n",
    "    Takes the path of annotated json file and \n",
    "    returns the texts as list and the aspects with annotated labels\n",
    "  '''\n",
    "  data_path = \"../../data/annotated/7/a2/sl_annotator_2.json\"\n",
    "  annotated_file = json.loads(open(os.path.join(cwd, data_path), 'r', encoding=\"utf-8\").read())\n",
    "  pos_count = 0\n",
    "  neg_count = 0\n",
    "  neu_count = 0\n",
    "\n",
    "  for i in range(len(annotated_file)):\n",
    "    line = annotated_file[i] #dictionary\n",
    "    tweet = line.get(\"data\") #string\n",
    "    tweet_label = line.get(\"label\") #list\n",
    "    tweet_label = tweet_label[0]\n",
    "    \n",
    "    if tweet_label == \"pos\":\n",
    "      pos_count += 1\n",
    "    elif tweet_label == \"neg\":\n",
    "      neg_count += 1\n",
    "    elif tweet_label == \"neu\":\n",
    "      neu_count += 1\n",
    "  \n",
    "  return pos_count,neg_count,neu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent_count,neg_sent_count,neu_sent_count = get_sentence_level_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "               ANNOTATED DATA EXPLORATION LOG                 \n",
      "==============================================================\n",
      "Number of tweets             : 3163\n",
      "Number of aspects            : 5000\n",
      "Number of unique aspects     : 980\n",
      "Number of positive aspects   : 635\n",
      "Number of negative aspects   : 750\n",
      "Number of neutral aspects    : 3615\n",
      "Number of positive sentence  : 257\n",
      "Number of negative sentence  : 496\n",
      "Number of neutral sentence   : 2410\n"
     ]
    }
   ],
   "source": [
    "print(\"==============================================================\")\n",
    "print(\"               ANNOTATED DATA EXPLORATION LOG                 \")\n",
    "print(\"==============================================================\")\n",
    "print(\"Number of tweets             : {}\".format(tweet_count))\n",
    "print(\"Number of aspects            : {}\".format(count_aspects))\n",
    "print(\"Number of unique aspects     : {}\".format(count_unique))\n",
    "print(\"Number of positive aspects   : {}\".format(pos_asp_count))\n",
    "print(\"Number of negative aspects   : {}\".format(neg_asp_count))\n",
    "print(\"Number of neutral aspects    : {}\".format(neu_asp_count))\n",
    "print(\"Number of positive sentence  : {}\".format(pos_sent_count))\n",
    "print(\"Number of negative sentence  : {}\".format(neg_sent_count))\n",
    "print(\"Number of neutral sentence   : {}\".format(neu_sent_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove noise from annotated aspects (do not run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_preprocessed_aspect(annotated_aspect_dict_list):\n",
    "  all_aspect_list = [] # store all aspect from tweets\n",
    "  noise_removed_aspect_list = [] # store all preprocessed aspects \n",
    "  \n",
    "  for i in range(len(annotated_aspect_dict_list)):\n",
    "    inner_list = annotated_aspect_dict_list[i] # list of {aspect:tag} combination of a tweet\n",
    "    tweet_aspect = [] # store the aspect of the tweet\n",
    "    for j in range(len(inner_list)):\n",
    "      inner_dict = inner_list[j] # dictionary of {aspect:tag} combination\n",
    "      for key in inner_dict:\n",
    "        tweet_aspect.append(key)\n",
    "    all_aspect_list.append(tweet_aspect)\n",
    "    \n",
    "  for i in tqdm(range(len(all_aspect_list))):\n",
    "    aspect_list = all_aspect_list[i]\n",
    "    cleaned_aspect_list = []\n",
    "\n",
    "    for j in range(len(aspect_list)):\n",
    "      aspect = aspect_list[j]\n",
    "      cleaned_aspect = preprocessing.noise_removal(aspect)\n",
    "      cleaned_aspect_list.append(cleaned_aspect)\n",
    "\n",
    "    noise_removed_aspect_list.append(cleaned_aspect_list)\n",
    "\n",
    "  return noise_removed_aspect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_list[1])\n",
    "print(annotated_aspect_dict_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count number of aspects for each polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count all aspects (unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspect_list = []\n",
    "\n",
    "for i in range(len(annotated_aspect_dict_list)):\n",
    "  annotated_aspect_list = annotated_aspect_dict_list[i]\n",
    "  inner_list = []\n",
    "  for j in range(len(annotated_aspect_list)):\n",
    "    inner_dict = annotated_aspect_list[j]\n",
    "    for aspect in inner_dict:\n",
    "      inner_list.append(aspect)\n",
    "  aspect_list.append(inner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_annotated_aspect_list = list(dict.fromkeys(annotated_aspect_list))\n",
    "unique_annotated_aspect_list = sorted(unique_annotated_aspect_list)\n",
    "print(\"There are {} aspects annotated\".format(len(unique_annotated_aspect_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aspects based on number of words\n",
    "def get_multi_word_aspect(unique_annotated_aspect_list):\n",
    "  single_word = []\n",
    "  mwa_two = []\n",
    "  mwa_greater_than_two = []\n",
    "  \n",
    "  for i in range(len(unique_annotated_aspect_list)):\n",
    "    aspect = unique_annotated_aspect_list[i]\n",
    "    number_of_words = len(aspect.split())\n",
    "    if number_of_words == 2:\n",
    "      mwa_two.append(aspect)\n",
    "    elif number_of_words > 2:\n",
    "      mwa_greater_than_two.append(aspect)\n",
    "    elif number_of_words < 2:\n",
    "      single_word.append(aspect)\n",
    "  \n",
    "  num_single_word = len(single_word)\n",
    "  num_mwa_two = len(mwa_two)\n",
    "  num_mwa_greater_than_two = len(mwa_greater_than_two)\n",
    "  \n",
    "  return (single_word,mwa_two,mwa_greater_than_two,num_single_word,num_mwa_two,num_mwa_greater_than_two)\n",
    "\n",
    "single_word,mwa_two,mwa_greater_than_two,num_single_word,num_mwa_two,num_mwa_greater_than_two = get_multi_word_aspect(unique_annotated_aspect_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================================================\")\n",
    "print(\"Number of single-word aspects: {}\".format(num_single_word))\n",
    "print(\"Single word annotated aspects:\")\n",
    "for i in range(len(single_word)):\n",
    "  print(single_word[i])\n",
    "\n",
    "print(\"\\n==================================================\")\n",
    "\n",
    "print(\"Number of double-word aspects: {}\".format(num_mwa_two))\n",
    "print(\"Single word annotated aspects:\")\n",
    "for i in range(len(mwa_two)):\n",
    "  print(mwa_two[i])\n",
    "  \n",
    "print(\"\\n==================================================\")\n",
    "  \n",
    "print(\"Number of greater than two-word aspects: {}\".format(num_mwa_greater_than_two))\n",
    "print(\"Single word annotated aspects:\")\n",
    "for i in range(len(mwa_greater_than_two)):\n",
    "  print(mwa_greater_than_two[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Top Annotated Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequency of annotated aspects\n",
    "count = utils.get_aspects_frequency(annotated_aspect_dict_list)\n",
    "sample_df = pd.DataFrame(count.most_common(50), columns=['Word', 'Frequency'])\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set_style(\"whitegrid\")\n",
    "ax = sns.barplot(x=\"Frequency\",y=\"Word\", data=sample_df.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def wc(data,bgcolor):\n",
    "    plt.figure(figsize = (80,80))\n",
    "    wc = WordCloud(background_color = bgcolor, max_words = 100,  max_font_size = 50)\n",
    "    wc.generate(' '.join(data))\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"wordcloud\"\n",
    "wc(annotated_aspect_list,bgcolor=\"black\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "310a519b676ac3702437d902e068b22df7f36fb88bfa9c8455f322a7fb8ed542"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
